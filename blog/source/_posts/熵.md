---
title: 熵
date: 2019-04-04 19:48:37
tags: [机器学习,熵,信息论]
categories: [机器学习,基础,熵]
keywords: [熵,信息熵,互信息,条件熵,KL-散度,交叉熵,相对熵]
description: 对熵的一些基本理解，免得学习时一脸懵逼。
image: /熵/0.jpg
---



## 信息熵(Entropy)

$$Entropy=\sum_{i=1}^nP_ilog\frac{1}{P_i}$$

　　理解：这个公式看着玄乎，但实际上非常符合人对信息的认知。

　　● 我们常常用信息量来表示一个事件包含的信息多少，而且非常直观的是：一个发生可能性越小的事件被我们获取后，我们的到信息量越大！所以**信息量的度量需要和事件发生的概率成反比**。

　　● 在我们的认知中，**信息量是能够相加的！**两个不相干事件的总信息量应该是他们各自信息量的求和。

　　有了以上两点要求，再结合独立事件联合概率 $P(x,y)=p(x)*p(y)$ 。哎，我们用 $log(\frac{1}{p})$ 来表示信息度量岂不美哉？没错！这就是**自信息 (self-information)**。而**整个Entropy则可以看成自信息的加权平均，是对所有可能发生的事件产生的信息量的期望**。 

　　到这里你可能会有疑惑，就这玩意真的有用？然而事实证明，这东西是真的牛逼。。。

　　信息熵是在信息论中提出的，是信息量的度量单位。而一条信息的信息量，我们可以对其进行理想化的无损压缩，将冗余的信息全部去除，这样一来压缩后的存储空间(bit)就能用来表示这条信息的信息量了。而对信息的压缩需要对信息进行编码。神奇的是——**当信息熵取2为底，单位为bit时，就是这条信息理想中的最小平均编码位数！！！** 也就是无损压缩理想中所能达到的极致。以最常见的Huffman编码为例，事件{1/4,1/4,1/4,1/4}可以编码为{0,10,110,111},$Entropy=2bit$而$实际平均编码=9/4bit$ ,没有达到理想平均编码。而如果事件{1/2,1/4,1/8,1/8}，$Entropy=1.75bit$而$实际平均编码=1.75bit$，正好达到理想平均编码。

　　实际上可以证明：

　　　　● **非均匀分布比均匀分布的熵要小。**

　　　　● **信息熵就是信息最短平均编码长度。**

　　　　● **情况种类越多，熵的上限越大。$0\le H(x) \le log(n)$ **

　　



## 交叉熵(Cross entropy)

$$Cross\ entropy=H(p,q)=\sum_xp(x)log\frac{1}{q(x)}$$

　　样本集的两个概率分布 p(x) 和 q(x)，其中 p(x) 为真实分布，q(x)为非真实分布。而**交叉熵则是用q分布的编码规则去编码p分布所得到的平均编码长度。**我们知道信息熵是编码最优解，所以交叉熵一定大于等于p分布的信息熵。而且**p和q分布越相似，交叉熵越小，直到等于真实分布的信息熵。



## 相对熵(Relative entropy)，KL散度(Kullback–Leibler divergence)

$$D_{KL}(p,q)=\sum_{x}p(x)log\frac{p(x)}{q(x)}＝\sum_xp(x)log\frac{1}{q(x)}-\sum_xp(x)log(\frac{1}{p(x)})=交叉熵-信息熵$$

　　KL散度就是交叉熵减信息熵的值。KL散度可以用来度量p和q分布的不相似程度，越不相似KL散度值越大，完全一致则值为0。



**★ 小结：**在机器学习中，一般用训练样本的标注近似数据的真实分布p。预测值则为非真实分布q。**损失函数：最小化交叉熵 = 最小化KL散度 = 最大似然估计！** 



##  条件熵(Conditional Entropy),互信息(Mutual information)

$$条件熵=H(X|Y)=\sum_{X,Y}p(x,y)log\frac{1}{p(x|y)}$$

$$互信息=I(X;Y)=\sum_{X,Y}p(x,y)log\frac{p(x,y)}{p(x)p(y)}$$

$$信息熵=条件熵+互信息$$

　　**条件熵：**在Y事件条件下，X事件的信息熵。

　　**互信息：**事件X和Y的相关性。

　　所以，当两个事件完全独立，即完全不相关，则互信息为0，条件熵为H(X)。
　　　　　当两个事件完全相关， p(x,y)=p(x),则互信息为H(X),条件熵为0。



**注意：条件熵和互信息中X,Y是两个不同的事件，而交叉熵和KL散度是同一个事件的不同分布。**

