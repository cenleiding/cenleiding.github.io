---
title: 神经网络ANN
date: 2018-10-19 20:31:59
tags: 机器学习，神经网络
categories: 机器学习
keywords: 神经网络,ANN,NN,机器学习
description: 什么是神经网络？对于神经网络的简单理解。
image: 神经网络ANN/ANN_1.jpg
---

![img](神经网络ANN/ANN_1.jpg)

## 神经网络是什么？

这里我也不想搞哪些麻烦的概念，只想简单的从直觉上去理解什么是神经网络。

首先来看一下神经网络的流程：数据从输入层输入，然后经过中间的隐藏层的激励函数，最后在输出层输出相应大小的结果。

### 方向一：从过程看

该部分内容参考[colah的博客。](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)

每一层的隐藏层都可以看成一个变换函数，输入数据经过一层隐藏层就相当于进行了一次变换。于是我们可以通过变换将不同平面的输入数据拉伸变换到一个平面并进行分离，这样对于输出结果我们只要进行简单的线性分割就能分类数据了。

比如我们现在要对以下的两个曲线进行分类：

![img](神经网络ANN/ANN_2.png)

我们发现无法使用线性分割的方式分类曲线，而获得一个“扭曲”的边界函数并不容易。于是通过神经网络我们将曲线变换为如下样子：

![img](/神经网络ANN/ANN_3.png)

这样我们就可以对神经网络的输出结果进行线性的分割，问题就变的很简单。当然这只是最简单情况，实际上对于较复杂的情况也一样：

![img](/神经网络ANN/ANN_4.GIF)

我们只要增加网络的隐藏层数量和每层的激励单元数量，就可以将一些更为复杂的函数分离开来。

但是这只是理想状态，往往我们并不能做到随便增加网络节点和层数因为计算量实在太复杂，而实际问题的数据又往往有着复杂的结构，如下图：

![img](/神经网络ANN/ANN_5.png)

现实数据往往是缠绕在一起的，要完全分离这样的数据需要十分复杂的神经网络。好在我们在实际应用时并不要求一定要得到完美的结果，于是我们可以通过神经网络将数据变换为以下样式：

![img](/神经网络ANN/ANN_6.png)

我们尽可能的让数据重叠的部分变少，然后分离数据，这样会产生误差，但是能较快的得到可用结果。



### 方向二：从结果看

上面讲的从过程看虽然十分形象，但是并不能帮助我们实现得到神经网络~毕竟我们并不知道应该怎么变化~~至少现在不能~~。但是从结果出发我们可以通过数学训练得到想要的神经网络！

对数据进行变形的过程，可以理解为对输入数据矩阵进行变换，每一个网络节点都是一次变换，我们的目标是强行将输入矩阵变的和目标结果矩阵相似。

而每一次变换都是一次矩阵的相乘，于是可以看成【输入矩阵*网络矩阵】=>【输出矩阵】，关键是确定网络矩阵中茫茫多的参数。

而输出矩阵和目标矩阵的相似度则可以用代价函数（cost -function）来表示，具体怎么设计代价函数看情况来定。这时我们可以将整体看成【输入矩阵 *输出矩阵 *代价函数】=>【代价值】。

毫无疑问这个代价值越小越好。而这个带价值是一个多元多次方程，如下图所示：

![img](/神经网络ANN/ANN_7.jpg)

我们的目标就是从中找到最低的那个点。

要求一个方程的最低点，就需要对每个参数求偏导，然后沿着梯度下降的方向变换参数，这就是**梯度下降算法 （Gradient Descent algorithm）GD。**而具体如何确定每个节点参数的变化，这就需要用到**反向传播算法 （Back Propagation algorithm）BP。** 

> 所以从结果来看：**神经网络的搭建就是假设一个高次的变化方程；训练就是凑变换函数参数。** 这样看来神经网络原理也就没什么，就是靠计算机的算力强行凑函数。。。 



## 梯度下降和反向传播是什么？

这部分内容还是主要参考[colah的博客。](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/) 和 [AGH](http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html)



>  **梯度下降**:

上面也讲到了，梯度下降算法的目的就是获得目标方程的最低点。而最低点的“斜率”是为0的，那么只要对方程求偏导，就能知道当前点的“斜率”，然后只要沿着这个斜率的反方向移动就能理论上到达最低点了。

过程如下图所示：

![img](/神经网络ANN/ANN_8.jpg)

**局部最低点问题：** 从上图我们可以看出，沿着梯度下降的方向移动并不一定能获得全局的最低点。因为神经网络创建的方程往往是很复杂的，有着大量的“谷底”，不同的起点最后掉落的“谷底”不一定相同。而这个起点取决于我们对参数的初始值设定。因此在实际训练网络时，有时对初始值设定进行一些“小操作”会有助于提高最后结果的准确度。。。嗯，所以神经网络很玄学！

![img](/神经网络ANN/ANN_9.png)

**学习速率问题：** 前面讲了要沿着梯度下降的方向移动，但是具体移动多少呢？首先很明显跟预测值与目标值的差距有关，两者差距越大那就要移动的越多，这就是**梯度误差（error gradient）**。但是如果直接根据这个误差来调整参数，发现参数变化幅度太大了，会导致结果一直在震荡（如右图所示），甚至直接就飞出这个“山谷”。。。因此需要一个值来缩小这个变化幅度，这就是**学习速率（learning rate）** ，学习速率往往是一个比较小的值如0.01，这样就能使得方程慢慢的向谷底移动。学习速率的选取是个需要注意的点，太小会导致下降的十分缓慢，训练网络需要很长时间，而太大又会引起“震荡”，学不到好的结果。



> **反向传播：**

知道了要进行梯度下降，但怎么具体操作是个问题，毕竟参数这么多，一个个求过来这个计算量不敢想~

于是就有了**反向传播算法**

这个部分一开始看公式一直很疑惑为什么要这么做？不就是个链式法则？这和直接求每个参数的偏导（正向传播）到底快在哪里？直到看了colah的讲解才恍然大悟，什么链式法则都不是重点，**避免重复计算才是反向传播算法的精髓！**

接下来膜拜colah大佬==>

首先看一下正常的求偏导是怎么求的：

![img](/神经网络ANN/ANN_10.png)

比如要求b对e的影响能力，我们用链式法则就能很简单的算出e对b的偏导值。

然后抽象一点，比如现在有如下网络，分别正向和反向的求一次偏导：

![img](/神经网络ANN/ANN_11.png)

正向（Forward-Mode）:以x为主，及表示了x**参数对每个节点的影响能力** 。

反向（Reverse-Mode）:以Z为主，及表示了Z**结果对每个节点的影响能力**。

嗯。。。。。看起来很有道理的样子，但是这有什么用呢？

那么就需要用例子体验一下两者的不同了！

![img](/神经网络ANN/ANN_12.png)

首先，两者都是要将各个路径的偏微分值计算出来的 ，毕竟这是基础，在网络中就是**路径权重值** 。。。

*向前传播*：是一个**路径遍历求和**的过程，要求b的影响要遍历一次，要求a的影响又要遍历一次，**同一路径会被多次计算**。而神经网络的参数和路径如此之多，要是每进行一次梯度下降就要对每个参数进行一次路径遍历，这个计算了就爆炸了！！！

*反向传播：*是一个**一次性扩散**的过程，可以看到向后传播直接确定了结果对每个节点的影响能力，**一条路径只会被计算一次**，这个几乎没有什么计算量。



看完不经感叹，反向传播原来这么简单，又这么有用。。。

最后跟着[AGH](http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html)上的反向传播图理一遍整个过程。

首先我们有一个简单的神经网络，如下图：

![img](/神经网络ANN/ANN_13.png)

然后输入x1,x2获得输出y，并计算得出误差δ：

![img](/神经网络ANN/ANN_14.png)

有了整体误差，那么就要开始反向传播了，进而获得每个节点的输出误差：

![img](/神经网络ANN/ANN_15.png)



最后就是对权重的调整，η就是学习速率：

![img](/神经网络ANN/ANN_16.png)

这里要注意一下，传递误差δ只是代表节点输出值的偏导，而我们的目标是路径权重w的偏导，中间还需要过度一下。

