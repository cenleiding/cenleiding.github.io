---
title: 神经网络ANN
date: 2018-10-19 20:31:59
tags: 机器学习，神经网络
categories: 机器学习
keywords: 神经网络,ANN,NN,机器学习
description: 什么是神经网络？对于神经网络的简单理解。
image: 神经网络ANN/ANN_1.jpg
---

![img](神经网络ANN/ANN_1.jpg)

## 神经网络是什么？

这里我也不想搞哪些麻烦的概念，只想简单的从直觉上去理解什么是神经网络。

首先来看一下神经网络的流程：数据从输入层输入，然后经过中间的隐藏层的激励函数，最后在输出层输出相应大小的结果。

###方向一：从过程看

该部分内容参考[colah的博客。](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)

每一层的隐藏层都可以看成一个变换函数，输入数据经过一层隐藏层就相当于进行了一次变换。于是我们可以通过变换将不同平面的输入数据拉伸变换到一个平面并进行分离，这样对于输出结果我们只要进行简单的线性分割就能分类数据了。

比如我们现在要对以下的两个曲线进行分类：

![img](神经网络ANN/ANN_2.png)

我们发现无法使用线性分割的方式分类曲线，而获得一个“扭曲”的边界函数并不容易。于是通过神经网络我们将曲线变换为如下样子：

![img](/神经网络ANN/ANN_3.png)

这样我们就可以对神经网络的输出结果进行线性的分割，问题就变的很简单。当然这只是最简单情况，实际上对于较复杂的情况也一样：

![img](/神经网络ANN/ANN_4.GIF)

我们只要增加网络的隐藏层数量和每层的激励单元数量，就可以将一些更为复杂的函数分离开来。

但是这只是理想状态，往往我们并不能做到随便增加网络节点和层数因为计算量实在太复杂，而实际问题的数据又往往有着复杂的结构，如下图：

![img](/神经网络ANN/ANN_5.png)

现实数据往往是缠绕在一起的，要完全分离这样的数据需要十分复杂的神经网络。好在我们在实际应用时并不要求一定要得到完美的结果，于是我们可以通过神经网络将数据变换为以下样式：

![img](/神经网络ANN/ANN_6.png)

我们尽可能的让数据重叠的部分变少，然后分离数据，这样会产生误差，但是能快速得到可用结果。



### 方向二：从结果看

上面讲的从过程看虽然十分形象，但是并不能帮助我们实现得到神经网络~毕竟我们并不知道应该怎么变化~~至少现在不能~~。但是从结果出发我们可以通过数学训练得到想要的神经网络！

对数据进行变形的过程，可以理解为对输入数据矩阵进行变换，每一个网络节点都是一次变换，我们的目标是强行将输入矩阵变的和目标结果矩阵相似。

而每一次变换都是一次矩阵的相乘，于是可以看成【输入矩阵*网络矩阵】=>【输出矩阵】，关键是确定网络矩阵中茫茫多的参数。

而输出矩阵和目标矩阵的相似度则可以用代价函数（cost -function）来表示，具体怎么设计代价函数看情况来定。这时我们可以将整体看成【输入矩阵 *输出矩阵 *代价函数】=>【代价值】。

毫无疑问这个代价值越小越好。而这个带价值是一个多元多次方程，如下图所示：

![img](/神经网络ANN/ANN_7.jpg)

我们的目标就是从中找到最低的那个点。

要求一个方程的最低点，就需要对每个参数求偏导，然后沿着梯度下降的方向变换参数，这就是**梯度下降算法 （Gradient Descent algorithm）GD。**而具体如何确定每个节点参数的变化，这就需要用到**反向传播算法 （Back Propagation algorithm）BP。** 

> 所以从结果来看：**神经网络的搭建就是假设一个高次的变化方程；训练就是凑变换函数参数。** 这样看来神经网络原理也就没什么，就是靠计算机的算力强行凑函数。。。 



## 梯度下降和反向传播是什么？

这部分内容还是主要参考[colah的博客。](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/) 和 [AGH](http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html)

TODO



