---
title: 百面机器学习
date: 2019-07-22 20:13:13
tags: [机器学习,百面机器学习]
categories: [笔记,百面机器学习笔记]
keywords: [机器学习,百面机器学习]
description: 哎，要找工作了，跟着百面机器学习看看有哪些知识遗漏点，临时抱抱佛脚~
image: 百面机器学习/0.jpg
---



# 百面机器学习

# １.特征工程

- **特征归一化**

  - 线性归一化：$x=\frac{x-x_{min}}{x_{max}-x_{min}}$

  - 零均值归一化：归一化为正太分布$z=\frac{x-\mu}{\sigma}$，需要距离度量的算法比较适合。

   **原因：**一般需要使用归一化的算法有两种：1.需要计算距离的，比如KNN，如果不做归一化，小值几乎没有影响。2.需要使用梯度下降的，在训练初期因为参数基本相同，梯度下降时大值的梯度会非常大，导致梯度下降的震荡，导致训练变慢，甚至无法收敛。3.像决策树和概率图模型则不需要归一化。
  
- **类别型特征**

  - 序号编码（ordinal Encoding）：类别之间具有大小关系。
  - 独热编码（one-hot Encoding）：类别之间没有大小 关系。

- **文本表示模型**

  - 词袋模型（Bag of Words）：简单的统计词频，忽略每个词的出现顺序。

  - TF-IDF：$TF-IDF=TF*IDF=本文词频*log(\frac{文章总数}{出现该词的文章数})$ ，用来表示一个词对于当前文本的重要程度。

  - 主题模型（Topic Model）：代表LDA，一种基于隐式Dirichlet先验概率分布的模型结构，无监督模型，整个模型非常的难，属于概率图范畴。简单来说模型计算两个概率分布：文档-主题的概率分布和主题-词汇的概率分布。训练时提供文本和主题数，输出各个文本的主题概率和每个主题下出现词的概率。

    注意点：LDA是个非常大的模型，只有当文本特征(词汇、词组)的系统太过复杂，又没有太多的人工分类经验或精力来做时才使用LDA。使用限制：1.文档足够多。2.文档足够长，词要足够多。3.词的种类要足够多的。4.词频足够大。所以LDA对文本的质量要求很严格，一般在特定专业领域比较好用，比如医疗文本分类。

  - word2vec：Skip-gram和CBOW。Hierarchical Softmax 和 Negative Sampling。



# 2. 模型评估

详见单章。



# 3. 经典算法

- **支持向量机**

  推导见单章。

  - 是否存在一组参数使误差为0?

     高斯核能拟合所有形状。 

- **逻辑回归**

  - 如何用逻辑回归处理多分类问题？

    - One-Vs-All：转变成N个二分类问题（是否是某一类）。类别比较多的情况下容易造成数据不平均的情况。
    - One-Vs-One：转变成C(N,2)个二分类问题（两两组合比较）。避免了数据不平衡问题，但分类器数量会很多。
    - 用softmax：改变了模型结构。

- **决策树**

  详见单章。

  - ID3-最大信息增益。C4.5-最大信息增益率。CART-最大基尼指数。
  - 预减枝、后减枝。
    - 预减枝：设置最大深度。设置节点最小样本数。设置分裂准确度提升阈值。容易造成欠拟合。
    
    - 后减枝：多种策略。。懒得看了。只知道从下往上判断该分类是否有用。
    
      

# 4. 降维

详见单章。

- PCA,LDA

# 5. 非监督学习

- K-means，GMM为代表的聚类算法。详见单章。

# 6.概率图

详见单章。























