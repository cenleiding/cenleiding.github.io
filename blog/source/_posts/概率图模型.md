---
title: 概率图模型
date: 2018-11-09 14:58:00
tags: 概率图模型 机器学习
categories: 机器学习 概率图模型
keywords: 概率图模型
description: 十分初浅的了解一下概率图模型
image: /概率图模型/PGM_1.JPG
---



## 什么是概率图模型？

在使用**命名实体识别**时用到了LSTM+CRF模型。因此需要了解一下CRF，嗯所以顺便**简单**的了解了一下**概率图模型**。发现概率图好难，哎只能了解一下皮毛了，实在没精力去十分详细的去学习了~~

**概率图模型(Probabilistic Graphical Model,PGM),简称图模型(Graphical Model,GM),是指一种用图结构来描述多元随机变量之间条件独立关系的概率模型，从而给研究高维空间中的概率模型带来了很大的便捷性。**



> 关键：**条件独立**

概率图模型的关键在于变量之间存在的**条件独立性！** 
比如对一个K维的随机向量 X = [X1，X2……XK] 建模，假设每个变量有m个取值且都不条件独立，那么要得到这个向量的**联合概率分布**情况则需要存储$$m^K-1$$ 个参数才行，(因为每个变量的概率都与其他所有的变量有关$$p(x)=\prod_{k=1}^{K}p(x_k|x1,……，x_{k-1})$$)，这个数据量就太大了是不可能存储的。但是如果存在**条件独立**那么存储量将大大减少，比如极端情况下各个变量两两都条件独立，那么我们只需要存储m*K个参数就可以了，就有了可行性！



> 三个基本问题：表示、学习、推断

**表示问题**：对于一个概率模型，如何通过图结构来描述变量之间的依赖关系。

**学习问题**：图模型的学习包括图结构的学习和参数的学习。

**推断问题**：在已知部分变量时，计算其他变量的后验概率分布。

![img](/概率图模型/PGM_2.png)



> PGM的一些见解

机器学习的一个核心任务是从观测到的数据中挖掘隐含的知识，而概率图模型是实现这一任务的一种很elegant，principled的手段。
PGM巧妙地结合了**图论**和**概率论**。**从图论的角度**，PGM是一个图，包含结点与边。结点可以分为两类：隐含结点和观测结点。边可以是有向的或者是无向的。**从概率论的角度**，PGM是一个概率分布，图中的结点对应于随机变量，边对应于随机变量的dependency或者correlation关系。
给定一个实际问题，我们通常会观测到一些数据，并且希望能够挖掘出隐含在数据中的知识。怎么用PGM实现呢？我们构建一个图，**用观测结点表示观测到的数据，用隐含结点表示潜在的知识，用边来描述知识与数据的相互关系，最后获得一个概率分布。**给定概率分布之后，通过进行两个任务：**inference** (给定观测结点，推断隐含结点的后验分布）和**learning**(学习这个概率分布的参数），来获取知识。
PGM的强大之处在于，不管数据和知识多复杂，我们的处理手段是一样的：**建一个图，定义一个概率分布，进行inference和learning。**这对于描述复杂的实际问题，构建大型的人工智能系统来说，是非常重要的。

以上采自[谢澎涛](https://www.zhihu.com/people/xpt-cmu)

因为**图模型中的每个变量一般有着明确的解释，变量之间依赖关系一般是人工来定义。** 所以概率图模型相对于其他模型往往有着**更好解释**的优点,但是如何构建一个好的图，如何进行inference和learning很麻烦。看了很多别人用概率图模型解决问题的感想，给了我一种很麻烦而且准确度也不怎么样的感觉！相比于神经网络什么的，概率图不是一种**黑箱子**问题，他无法通过简单的调调参数就能解决问题，他更加的贴近人的正常思维，而这正是他的优势和劣势所在！



## 极大似然估计MLE && 最大后验概率估计MAP

> 极大似然估计 Maximum Likelihood Estimate
>
> 最大后验概率估计 Maximum A Posteriori estimation

在看概率图模型时，经常会看到最大似然估计和最大后验估计。因为概率论的东西基本全还给老师了，所以看的一愣一愣的。没办法只好查资料补起来，说实话这两个东西初看看挺简单的，以为已经理解了，但是当我尝试去分析一个问题时又会有点茫然。。真是头大，到写这些东西，我仍觉得并没有真正理解这两个概念。。。。。



### 起源

**极大似然估计（MLE）--  频率学派**

**最大后验概率估计（MAP）-- 贝叶斯派**

事实上，我对这两个学派并没有什么兴趣。他们只是根据自己的理解分别提出了各自的观点而已。

频率派认为**参数是个客观存在的固定值**，而贝叶斯派则认为**参数是服从一个分布的随机值**。

相比于频率派贝叶斯派增加了**参数的先验分布**，然后根据数据计算**参数的后验分布** 。这个先验分布**一般根据历史数据统计得到，也可以人为经验给定**，这使的结果并不完全依赖于样本数据，**于是往往用后验估计更加可靠。**但问题在于怎么获得这个先验假设？好的先验假设确实有用，但拍拍脑袋想出来的先验假设就只能呵呵了~

另外**这两个学派至今未能达成共识！** 想起自己刚看这两个估计时傻傻的想把他们合在一起理解真是头大。。



### 用途

两者的用途一致：**对参数进行估计！**

即**模型已定，参数未知** 。注意**除可观察值之外的都可以是参数，即不可见值**



### 极大似然估计MLE

$P(样本D|参数\theta)$

**核心目标：找到参数θ的一个估计值，使得当前样本出现的可能性最大。**

假设有一个样本集合D，且这些样本都是独立的，则参数θ对于数据集D的似然为：

$$P(D|\theta) = P(x1,x2…xn|\theta) = \prod_{x\in{D}}P(x|\theta)$$

由于实际使用中，P(x|θ)往往比较小，连乘容易造成**浮点运算下溢**，而且为了**求导方便**，通常使用**对数似然(log-likelihood):**

$LL(\theta) = log P(D|\theta) = \sum_{x\in{D}} log P(x|\theta)$

此时，参数θ的**极大似然估计**为：

$\hat\theta = arg_{\theta}maxLL(\theta)$

而这个最大值可以通过**求导得出** 。



> 例子

以最为简单的投硬币为例，现在有一个硬币，如果正面朝上记为1，反面朝上记为0，抛10次的结果如下：1111100011=>7正3反。我们的目标就是求出硬币向上的概率。

首先这个问题有10个样本，模型则是二项分布，参数为向上概率θ。

对于每个样本有：$P(x|\theta) = \theta^{x}*(1-\theta)^{1-x}$

那么对于整个数据有似然函数：$P(X|\theta) = \theta^{7}*(1-\theta)^{3}$

最后转为对数似然并求导，令导数为0，可以得到θ=0.7。与我们的预期一致。

**极大似然估计的问题：** 可以看出通过极大似然估计得到的结果**严重依赖于样本**，正面向上的概率为0.7与我们的日常经验不符，这就是极大似然的问题所在。



###　最大后验概率估计（MAP）

极大似然估计的问题就是太依赖样本了，于是最大后验概率估计添加了一项**先验概率**，可以将其看成**惩罚**，来平衡由样本得到的概率。

$P(参数\theta|样本D)$

最大后验求的是在样本下，参数θ的最大值。嗯，并不能直观的理解。使用**贝叶斯公式**转换看看。

**贝叶斯公式：** $P(x|y) = \frac{P(y|x)P(x)}{P(y)}$

转换后：$P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}$

P(D|θ)：似然概率，P(θ)：先验概率，P(D)：能用全概率公式计算，但不需要，因为我们只关心使值最大的θ，而不是值的本身。

于是求最大后验概率估计变的和求极大似然估计差不多了，就是多了一个先验概率。

> 例子

任然是刚才的投硬币，我们发现0.7并不符合常识。所以我们加一个**先验概率：硬币是正常的,向上的概率符合最大值取在0.5处的Beta分布。** 这样再求最大值，θ就会处于0.5~0.7，被修正了。



### 两者的关系

很明显，**最大后验可以认为是极大似然通过先验分布进行了修正。**

特别的，**可以将极大似然看成先验为均匀分布的最大后验！**

特别的，**当样本数十分大时，先验概率将几乎没有影响，极大似然的结果与最大后验一致**